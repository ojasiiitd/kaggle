{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN , LSTM , Dense , Activation , Dropout\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"LYRICS_DATASET.csv\")[\"Lyrics\"]\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GIT CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 430994\n",
      "total chars: 73\n"
     ]
    }
   ],
   "source": [
    "text = df.str.cat(sep='\\n').lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "# # Create a sorted list of the characters\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 143652\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40 # The window size\n",
    "step = 3 # The steps between the windows\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen]) # range from current index i for max length characters \n",
    "    next_chars.append(text[i + maxlen]) # the next character after that \n",
    "sentences = np.array(sentences)\n",
    "next_chars = np.array(next_chars)\n",
    "print('Number of sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdata(sentences, next_chars):\n",
    "    X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "    length = len(sentences)\n",
    "    index = 0\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "    return X, y\n",
    "\n",
    "def generator(sentences, next_chars, batch_size):\n",
    "    X = np.zeros((batch_size, maxlen, len(chars)), dtype=np.bool)\n",
    "    y = np.zeros((batch_size, len(chars)), dtype=np.bool)\n",
    "    length = len(sentences)\n",
    "    index = 0\n",
    "    while True:\n",
    "        if index + batch_size >= length:\n",
    "            index = 0\n",
    "        X.fill(0)\n",
    "        y.fill(0)\n",
    "        for i in range(batch_size):\n",
    "            sentence = sentences[index]\n",
    "            for t, char in enumerate(sentence):\n",
    "                X[i, t, char_indices[char]] = 1\n",
    "            y[i, char_indices[next_chars[i]]] = 1\n",
    "            index = index + 1\n",
    "        yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Compiling model complete...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "print(\"Compiling model complete...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/3\n",
      "1123/1123 [==============================] - 72s 64ms/step - loss: 2.4445\n",
      "Epoch 2/3\n",
      "1123/1123 [==============================] - 80s 71ms/step - loss: 2.1535\n",
      "Epoch 3/3\n",
      "1123/1123 [==============================] - 78s 70ms/step - loss: 2.0415\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "X, y = getdata(sentences, next_chars)\n",
    "\n",
    "# The training\n",
    "print('Training...')\n",
    "batch_size = 128\n",
    "\n",
    "# Use the below command if you want to use the generator\n",
    "# history = model.fit_generator(generator(sentences, next_chars, batch_size),steps_per_epoch=12800, epochs=10)\n",
    "\n",
    "# Use this if they all fit into memory\n",
    "history = model.fit(X, y,batch_size=128, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.46973545e-03 7.41822541e-01 1.34595466e-04 1.04182679e-03\n",
      " 5.37435881e-06 8.24598689e-03 6.14967212e-05 4.44581034e-03\n",
      " 1.19459185e-06 8.84427205e-02 2.49962509e-03 1.58483069e-03\n",
      " 2.17153556e-05 2.44890362e-05 5.03982665e-05 1.71404099e-05\n",
      " 9.51096899e-06 1.15685634e-05 1.90241790e-05 5.66784229e-06\n",
      " 1.57579016e-05 1.91032814e-05 1.26093812e-06 4.44709694e-05\n",
      " 8.60144282e-05 6.23757113e-03 1.11675433e-06 2.48044240e-03\n",
      " 2.12513097e-03 1.13536348e-03 7.11568445e-03 1.80438645e-02\n",
      " 8.41407862e-04 2.12555658e-03 2.08464637e-03 8.26658402e-03\n",
      " 1.17076044e-04 1.73337420e-03 3.31662316e-03 9.34394891e-04\n",
      " 6.11539744e-03 1.87885966e-02 1.71499036e-03 6.63707033e-05\n",
      " 4.77941800e-03 3.41566615e-02 1.86528452e-02 2.90284195e-04\n",
      " 3.49330221e-04 9.21573781e-04 1.36823684e-04 4.73968545e-03\n",
      " 1.95179513e-04 5.07472066e-07 4.12755435e-06 7.81688357e-07\n",
      " 6.39239033e-07 5.98795305e-06 8.54322298e-06 2.73509886e-05\n",
      " 2.04082899e-05 5.19566629e-06 1.57186165e-04 4.17540423e-06\n",
      " 2.44236056e-04 1.59803531e-05 7.96352047e-04 7.93541312e-06\n",
      " 6.35270626e-05 1.48366462e-05 3.64709558e-05 3.72760660e-05\n",
      " 6.49059189e-07]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'dance all night\\ndance all day\\ndance away'\n",
    "x = np.zeros((1, maxlen, len(chars)))\n",
    "for t, char in enumerate(sentence):\n",
    "    x[0, t, char_indices[char]] = 1.\n",
    "    \n",
    "print(model.predict(x, verbose=0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance:  0.25\n",
      "[2.46973545e-03 7.41822541e-01 1.34595466e-04 1.04182679e-03\n",
      " 5.37435881e-06 8.24598689e-03 6.14967212e-05 4.44581034e-03\n",
      " 1.19459185e-06 8.84427205e-02 2.49962509e-03 1.58483069e-03\n",
      " 2.17153556e-05 2.44890362e-05 5.03982665e-05 1.71404099e-05\n",
      " 9.51096899e-06 1.15685634e-05 1.90241790e-05 5.66784229e-06\n",
      " 1.57579016e-05 1.91032814e-05 1.26093812e-06 4.44709694e-05\n",
      " 8.60144282e-05 6.23757113e-03 1.11675433e-06 2.48044240e-03\n",
      " 2.12513097e-03 1.13536348e-03 7.11568445e-03 1.80438645e-02\n",
      " 8.41407862e-04 2.12555658e-03 2.08464637e-03 8.26658402e-03\n",
      " 1.17076044e-04 1.73337420e-03 3.31662316e-03 9.34394891e-04\n",
      " 6.11539744e-03 1.87885966e-02 1.71499036e-03 6.63707033e-05\n",
      " 4.77941800e-03 3.41566615e-02 1.86528452e-02 2.90284195e-04\n",
      " 3.49330221e-04 9.21573781e-04 1.36823684e-04 4.73968545e-03\n",
      " 1.95179513e-04 5.07472066e-07 4.12755435e-06 7.81688357e-07\n",
      " 6.39239033e-07 5.98795305e-06 8.54322298e-06 2.73509886e-05\n",
      " 2.04082899e-05 5.19566629e-06 1.57186165e-04 4.17540423e-06\n",
      " 2.44236056e-04 1.59803531e-05 7.96352047e-04 7.93541312e-06\n",
      " 6.35270626e-05 1.48366462e-05 3.64709558e-05 3.72760660e-05\n",
      " 6.49059189e-07]\n"
     ]
    }
   ],
   "source": [
    "variance = 0.25\n",
    "print('Variance: ', variance)\n",
    "\n",
    "generated = ''\n",
    "original = sentence\n",
    "window = sentence\n",
    "# Predict the next 400 characters based on the seed\n",
    "for i in range(400):\n",
    "    x = np.zeros((1, maxlen, len(chars)))\n",
    "    for t, char in enumerate(window):\n",
    "        x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "    next_index = sample(preds, variance)\n",
    "    next_char = indices_char[next_index]\n",
    "\n",
    "    generated += next_char\n",
    "    window = window[1:] + next_char\n",
    "\n",
    "print(original + generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyr_alpha = data\n",
    "features = []\n",
    "i=0\n",
    "for curSong in lyr_alpha:\n",
    "    curSong = str(curSong).lower().split()\n",
    "    features.append(curSong)\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when',\n",
       " 'the',\n",
       " 'light',\n",
       " 'revealed',\n",
       " 'the',\n",
       " 'dream',\n",
       " 'opened',\n",
       " 'the',\n",
       " 'door',\n",
       " 'hoping',\n",
       " 'to',\n",
       " 'lean',\n",
       " 'and',\n",
       " 'i',\n",
       " 'gave',\n",
       " 'my',\n",
       " 'own',\n",
       " 'life',\n",
       " 'and',\n",
       " 'i',\n",
       " 'gave',\n",
       " 'it',\n",
       " 'all',\n",
       " 'to',\n",
       " 'the',\n",
       " 'wind',\n",
       " 'all',\n",
       " 'for',\n",
       " 'love',\n",
       " 'gave',\n",
       " 'that',\n",
       " 'a',\n",
       " 'try',\n",
       " 'head',\n",
       " 'on',\n",
       " 'the',\n",
       " 'floor',\n",
       " 'most',\n",
       " 'of',\n",
       " 'the',\n",
       " 'time',\n",
       " 'and',\n",
       " 'i',\n",
       " 'gave',\n",
       " 'my',\n",
       " 'own',\n",
       " 'life',\n",
       " 'and',\n",
       " 'i',\n",
       " 'gave',\n",
       " 'it',\n",
       " 'all',\n",
       " 'to',\n",
       " 'the',\n",
       " 'wind',\n",
       " 'love',\n",
       " 'loving',\n",
       " 'is',\n",
       " 'a',\n",
       " 'new',\n",
       " 'part',\n",
       " 'of',\n",
       " 'and',\n",
       " 'up',\n",
       " 'until',\n",
       " 'today',\n",
       " 'i',\n",
       " 'seemed',\n",
       " 'to',\n",
       " 'get',\n",
       " 'away',\n",
       " 'i',\n",
       " 'can',\n",
       " 'see',\n",
       " 'the',\n",
       " 'stars',\n",
       " 'above',\n",
       " 'swirling',\n",
       " 'around',\n",
       " 'to',\n",
       " 'say',\n",
       " 'out',\n",
       " 'went',\n",
       " 'the',\n",
       " 'light',\n",
       " 'as',\n",
       " 'i',\n",
       " 'leapt',\n",
       " 'through',\n",
       " 'the',\n",
       " 'door',\n",
       " 'and',\n",
       " 'i',\n",
       " 'followed',\n",
       " 'it',\n",
       " 'home',\n",
       " 'and',\n",
       " 'i',\n",
       " \"didn't\",\n",
       " 'know',\n",
       " 'why',\n",
       " 'i',\n",
       " \"didn't\",\n",
       " 'know',\n",
       " 'why',\n",
       " 'all',\n",
       " 'the',\n",
       " 'nights',\n",
       " \"i'd\",\n",
       " 'been',\n",
       " 'afraid',\n",
       " 'living',\n",
       " 'life',\n",
       " 'just',\n",
       " 'to',\n",
       " 'fade',\n",
       " 'away',\n",
       " 'but',\n",
       " 'all',\n",
       " 'the',\n",
       " 'light',\n",
       " 'laying',\n",
       " 'in',\n",
       " 'your',\n",
       " 'eyes',\n",
       " 'left',\n",
       " 'me',\n",
       " 'stunned',\n",
       " 'in',\n",
       " 'a',\n",
       " 'pale',\n",
       " 'surprise',\n",
       " 'woke',\n",
       " 'up',\n",
       " 'within',\n",
       " 'that',\n",
       " 'morning',\n",
       " 'just',\n",
       " 'to',\n",
       " 'turn',\n",
       " 'back',\n",
       " 'time',\n",
       " 'and',\n",
       " 'get',\n",
       " 'back',\n",
       " 'in',\n",
       " 'line',\n",
       " 'her',\n",
       " 'old',\n",
       " \"man's\",\n",
       " 'driving',\n",
       " 'the',\n",
       " 'whip',\n",
       " 'too',\n",
       " 'much',\n",
       " 'to',\n",
       " 'bear',\n",
       " 'rolled',\n",
       " 'up',\n",
       " 'a',\n",
       " 'spliff',\n",
       " 'and',\n",
       " 'i',\n",
       " 'gave',\n",
       " 'my',\n",
       " 'own',\n",
       " 'life',\n",
       " 'and',\n",
       " 'i',\n",
       " 'gave',\n",
       " 'it',\n",
       " 'all',\n",
       " 'to',\n",
       " 'the',\n",
       " 'wind',\n",
       " 'quite',\n",
       " 'the',\n",
       " 'type',\n",
       " 'to',\n",
       " 'treat',\n",
       " 'you',\n",
       " 'unkind',\n",
       " 'and',\n",
       " 'take',\n",
       " 'all',\n",
       " 'your',\n",
       " 'love',\n",
       " 'most',\n",
       " 'of',\n",
       " 'the',\n",
       " 'time',\n",
       " 'and',\n",
       " 'i',\n",
       " 'gave',\n",
       " 'my',\n",
       " 'own',\n",
       " 'life',\n",
       " 'and',\n",
       " 'i',\n",
       " 'gave',\n",
       " 'it',\n",
       " 'all',\n",
       " 'to',\n",
       " 'the',\n",
       " 'wind',\n",
       " 'love',\n",
       " 'loving',\n",
       " 'is',\n",
       " 'a',\n",
       " 'new',\n",
       " 'part',\n",
       " 'of',\n",
       " 'and',\n",
       " 'up',\n",
       " 'until',\n",
       " 'today',\n",
       " 'i',\n",
       " 'seemed',\n",
       " 'to',\n",
       " 'get',\n",
       " 'away',\n",
       " 'i',\n",
       " 'can',\n",
       " 'see',\n",
       " 'the',\n",
       " 'stars',\n",
       " 'above',\n",
       " 'swirling',\n",
       " 'around',\n",
       " 'to',\n",
       " 'say',\n",
       " 'out',\n",
       " 'went',\n",
       " 'the',\n",
       " 'light',\n",
       " 'as',\n",
       " 'i',\n",
       " 'leapt',\n",
       " 'through',\n",
       " 'the',\n",
       " 'door',\n",
       " 'and',\n",
       " 'i',\n",
       " 'followed',\n",
       " 'it',\n",
       " 'home',\n",
       " 'and',\n",
       " 'i',\n",
       " \"didn't\",\n",
       " 'know',\n",
       " 'why',\n",
       " 'i',\n",
       " \"didn't\",\n",
       " 'know',\n",
       " 'why',\n",
       " 'all',\n",
       " 'the',\n",
       " 'nights',\n",
       " \"i'd\",\n",
       " 'been',\n",
       " 'afraid',\n",
       " 'living',\n",
       " 'life',\n",
       " 'just',\n",
       " 'to',\n",
       " 'fade',\n",
       " 'away',\n",
       " 'but',\n",
       " 'all',\n",
       " 'the',\n",
       " 'light',\n",
       " 'laying',\n",
       " 'in',\n",
       " 'your',\n",
       " 'eyes',\n",
       " 'left',\n",
       " 'me',\n",
       " 'stunned',\n",
       " 'in',\n",
       " 'a',\n",
       " 'pale',\n",
       " 'surprise',\n",
       " 'woke',\n",
       " 'up',\n",
       " 'within',\n",
       " 'that',\n",
       " 'morning',\n",
       " 'just',\n",
       " 'to',\n",
       " 'turn',\n",
       " 'back',\n",
       " 'time',\n",
       " 'and',\n",
       " 'get',\n",
       " 'back',\n",
       " 'in',\n",
       " 'line']"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_ord = []\n",
    "for curSong in features:\n",
    "    words_ord = []\n",
    "    for word in curSong:\n",
    "        s = [ord(ch) for ch in word]\n",
    "        words_ord.append(s)\n",
    "    features_ord.append(words_ord)\n",
    "len(features)\n",
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for song in features:\n",
    "    for w in range(len(song)):\n",
    "        vocab.append(song[w])\n",
    "    for w in range(len(song)-1):\n",
    "        vocab.append(song[w] + \" \" + song[w+1])\n",
    "    for w in range(len(song)-2):\n",
    "        vocab.append(song[w] + \" \" + song[w+1] + \" \" + song[w+2])\n",
    "    for w in range(len(song)-3):\n",
    "        vocab.append(song[w] + \" \" + song[w+1] + \" \" + song[w+2] + \" \" + song[w+3])\n",
    "word_set = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160655"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict = {}\n",
    "c = 1\n",
    "for w in word_set:\n",
    "    vocab_dict[w] = c\n",
    "    c += 1\n",
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(data , window , target):\n",
    "    dataX , dataY = [] , []\n",
    "    for song in range(len(data)):\n",
    "        x = []\n",
    "        y = []\n",
    "        for word in range(len(data[song]) - window):\n",
    "            tr = \"\".join(i+\" \" for i in data[song][word : word+window]).strip()\n",
    "            te = \"\".join(i+\" \" for i in data[song][word+window : word+window+target]).strip()\n",
    "            \n",
    "            x.append(vocab_dict[tr])\n",
    "            y.append(vocab_dict[te])\n",
    "\n",
    "        dataX.append(x)\n",
    "        dataY.append(y)\n",
    "    \n",
    "    return dataX , dataY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[147872]],\n",
       " [[30016]],\n",
       " [[90795]],\n",
       " [[70505]],\n",
       " [[122425]],\n",
       " [[67682]],\n",
       " [[21279]],\n",
       " [[139969]],\n",
       " [[102517]],\n",
       " [[108350]],\n",
       " [[27692]],\n",
       " [[44302]],\n",
       " [[71679]],\n",
       " [[150060]],\n",
       " [[68088]],\n",
       " [[83226]],\n",
       " [[93830]],\n",
       " [[13963]],\n",
       " [[106712]],\n",
       " [[112762]],\n",
       " [[86860]],\n",
       " [[26462]],\n",
       " [[77626]],\n",
       " [[32086]],\n",
       " [[23711]],\n",
       " [[103173]],\n",
       " [[106453]],\n",
       " [[57231]],\n",
       " [[62317]],\n",
       " [[51079]],\n",
       " [[44552]],\n",
       " [[59926]],\n",
       " [[74699]],\n",
       " [[155420]],\n",
       " [[73498]],\n",
       " [[160020]],\n",
       " [[97643]],\n",
       " [[17345]],\n",
       " [[21624]],\n",
       " [[103066]],\n",
       " [[77355]],\n",
       " [[71679]],\n",
       " [[150060]],\n",
       " [[68088]],\n",
       " [[83226]],\n",
       " [[93830]],\n",
       " [[13963]],\n",
       " [[106712]],\n",
       " [[112762]],\n",
       " [[86860]],\n",
       " [[26462]],\n",
       " [[77626]],\n",
       " [[3148]],\n",
       " [[101097]],\n",
       " [[94474]],\n",
       " [[82182]],\n",
       " [[16225]],\n",
       " [[59164]],\n",
       " [[131276]],\n",
       " [[21372]],\n",
       " [[48107]],\n",
       " [[3806]],\n",
       " [[10995]],\n",
       " [[10531]],\n",
       " [[15769]],\n",
       " [[55743]],\n",
       " [[48725]],\n",
       " [[42163]],\n",
       " [[128403]],\n",
       " [[90454]],\n",
       " [[99707]],\n",
       " [[117653]],\n",
       " [[50696]],\n",
       " [[106596]],\n",
       " [[107991]],\n",
       " [[81129]],\n",
       " [[113998]],\n",
       " [[74790]],\n",
       " [[145875]],\n",
       " [[45108]],\n",
       " [[11900]],\n",
       " [[68466]],\n",
       " [[128360]],\n",
       " [[31063]],\n",
       " [[37535]],\n",
       " [[1700]],\n",
       " [[100159]],\n",
       " [[43236]],\n",
       " [[38607]],\n",
       " [[119533]],\n",
       " [[140363]],\n",
       " [[49108]],\n",
       " [[34030]],\n",
       " [[19945]],\n",
       " [[107877]],\n",
       " [[101292]],\n",
       " [[140874]],\n",
       " [[36762]],\n",
       " [[60503]],\n",
       " [[130261]],\n",
       " [[87859]],\n",
       " [[36762]],\n",
       " [[108710]],\n",
       " [[143493]],\n",
       " [[97833]],\n",
       " [[67159]],\n",
       " [[101493]],\n",
       " [[146695]],\n",
       " [[115643]],\n",
       " [[73918]],\n",
       " [[46225]],\n",
       " [[114745]],\n",
       " [[37962]],\n",
       " [[31748]],\n",
       " [[113257]],\n",
       " [[156302]],\n",
       " [[6096]],\n",
       " [[90366]],\n",
       " [[66276]],\n",
       " [[104359]],\n",
       " [[39921]],\n",
       " [[123832]],\n",
       " [[42243]],\n",
       " [[47396]],\n",
       " [[90297]],\n",
       " [[21731]],\n",
       " [[105663]],\n",
       " [[13939]],\n",
       " [[870]],\n",
       " [[91376]],\n",
       " [[130640]],\n",
       " [[141950]],\n",
       " [[8834]],\n",
       " [[125317]],\n",
       " [[84714]],\n",
       " [[24560]],\n",
       " [[101858]],\n",
       " [[130192]],\n",
       " [[43974]],\n",
       " [[37644]],\n",
       " [[56296]],\n",
       " [[107299]],\n",
       " [[99005]],\n",
       " [[52707]],\n",
       " [[13850]],\n",
       " [[85954]],\n",
       " [[93459]],\n",
       " [[64663]],\n",
       " [[157398]],\n",
       " [[116994]],\n",
       " [[41837]],\n",
       " [[20235]],\n",
       " [[144464]],\n",
       " [[43211]],\n",
       " [[115990]],\n",
       " [[119509]],\n",
       " [[104250]],\n",
       " [[133369]],\n",
       " [[93185]],\n",
       " [[148222]],\n",
       " [[35519]],\n",
       " [[71679]],\n",
       " [[150060]],\n",
       " [[68088]],\n",
       " [[83226]],\n",
       " [[93830]],\n",
       " [[13963]],\n",
       " [[106712]],\n",
       " [[112762]],\n",
       " [[86860]],\n",
       " [[26462]],\n",
       " [[77626]],\n",
       " [[130749]],\n",
       " [[35604]],\n",
       " [[31770]],\n",
       " [[90951]],\n",
       " [[58980]],\n",
       " [[153159]],\n",
       " [[128630]],\n",
       " [[58844]],\n",
       " [[10710]],\n",
       " [[90106]],\n",
       " [[68777]],\n",
       " [[105760]],\n",
       " [[69308]],\n",
       " [[107850]],\n",
       " [[150525]],\n",
       " [[17345]],\n",
       " [[21624]],\n",
       " [[103066]],\n",
       " [[77355]],\n",
       " [[71679]],\n",
       " [[150060]],\n",
       " [[68088]],\n",
       " [[83226]],\n",
       " [[93830]],\n",
       " [[13963]],\n",
       " [[106712]],\n",
       " [[112762]],\n",
       " [[86860]],\n",
       " [[26462]],\n",
       " [[77626]],\n",
       " [[3148]],\n",
       " [[101097]],\n",
       " [[94474]],\n",
       " [[82182]],\n",
       " [[16225]],\n",
       " [[59164]],\n",
       " [[131276]],\n",
       " [[21372]],\n",
       " [[48107]],\n",
       " [[3806]],\n",
       " [[10995]],\n",
       " [[10531]],\n",
       " [[15769]],\n",
       " [[55743]],\n",
       " [[48725]],\n",
       " [[42163]],\n",
       " [[128403]],\n",
       " [[90454]],\n",
       " [[99707]],\n",
       " [[117653]],\n",
       " [[50696]],\n",
       " [[106596]],\n",
       " [[107991]],\n",
       " [[81129]],\n",
       " [[113998]],\n",
       " [[74790]],\n",
       " [[145875]],\n",
       " [[45108]],\n",
       " [[11900]],\n",
       " [[68466]],\n",
       " [[128360]],\n",
       " [[31063]],\n",
       " [[37535]],\n",
       " [[1700]],\n",
       " [[100159]],\n",
       " [[43236]],\n",
       " [[38607]],\n",
       " [[119533]],\n",
       " [[140363]],\n",
       " [[49108]],\n",
       " [[34030]],\n",
       " [[19945]],\n",
       " [[107877]],\n",
       " [[101292]],\n",
       " [[140874]],\n",
       " [[36762]],\n",
       " [[60503]],\n",
       " [[130261]],\n",
       " [[87859]],\n",
       " [[36762]],\n",
       " [[108710]],\n",
       " [[143493]],\n",
       " [[97833]],\n",
       " [[67159]],\n",
       " [[101493]],\n",
       " [[146695]],\n",
       " [[115643]],\n",
       " [[73918]],\n",
       " [[46225]],\n",
       " [[114745]],\n",
       " [[37962]],\n",
       " [[31748]],\n",
       " [[113257]],\n",
       " [[156302]],\n",
       " [[6096]],\n",
       " [[90366]],\n",
       " [[66276]],\n",
       " [[104359]],\n",
       " [[39921]],\n",
       " [[123832]],\n",
       " [[42243]],\n",
       " [[47396]],\n",
       " [[90297]],\n",
       " [[21731]],\n",
       " [[105663]],\n",
       " [[13939]],\n",
       " [[870]],\n",
       " [[91376]],\n",
       " [[130640]],\n",
       " [[141950]],\n",
       " [[8834]],\n",
       " [[125317]],\n",
       " [[84714]],\n",
       " [[24560]],\n",
       " [[101858]],\n",
       " [[130192]],\n",
       " [[43974]],\n",
       " [[37644]],\n",
       " [[56296]],\n",
       " [[107299]],\n",
       " [[99005]]]"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window = 4\n",
    "target = 2\n",
    "train , test = train_test(features , window , target)\n",
    "for i in range(len(train)):\n",
    "    for j in range(len(train[i])):\n",
    "        train[i][j] = [train[i][j]]\n",
    "for i in range(len(test)):\n",
    "    for j in range(len(train[i])):\n",
    "        train[i][j] = [train[i][j]]\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ojas3/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  after removing the cwd from sys.path.\n",
      "/home/ojas3/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(347, 347)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clipped_train = []\n",
    "for i in train:\n",
    "    clipped_train.append(np.array(i[:50]))\n",
    "clipped_train = np.array(clipped_train)\n",
    "\n",
    "clipped_test = []\n",
    "for i in test:\n",
    "    clipped_test.append(np.array(i[:50]))\n",
    "clipped_test = np.array(clipped_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 1)"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[[1],[-1]],[[2],[-1]]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 347 into shape (347,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-321-28d809f5445e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mytr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mxtr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    297\u001b[0m            [5, 6]])\n\u001b[1;32m    298\u001b[0m     \"\"\"\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 347 into shape (347,4)"
     ]
    }
   ],
   "source": [
    "xtr = np.reshape(train , (train.shape[0] , 4))\n",
    "ytr = np.reshape(test , (test.shape[0] , 2))\n",
    "xtr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "50/50 [==============================] - 0s 895us/step - loss: 9009513472.0000 - accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "50/50 [==============================] - 0s 895us/step - loss: 9009478656.0000 - accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "50/50 [==============================] - 0s 901us/step - loss: 9009440768.0000 - accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "50/50 [==============================] - 0s 916us/step - loss: 9009406976.0000 - accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "50/50 [==============================] - 0s 911us/step - loss: 9009373184.0000 - accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "50/50 [==============================] - 0s 900us/step - loss: 9009336320.0000 - accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      "50/50 [==============================] - 0s 817us/step - loss: 9009302528.0000 - accuracy: 0.0000e+00\n",
      "Epoch 8/20\n",
      "50/50 [==============================] - 0s 934us/step - loss: 9009267712.0000 - accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "50/50 [==============================] - 0s 946us/step - loss: 9009230848.0000 - accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "50/50 [==============================] - 0s 853us/step - loss: 9009197056.0000 - accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "50/50 [==============================] - 0s 862us/step - loss: 9009161216.0000 - accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "50/50 [==============================] - 0s 951us/step - loss: 9009125376.0000 - accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      "50/50 [==============================] - 0s 954us/step - loss: 9009089536.0000 - accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      "50/50 [==============================] - 0s 969us/step - loss: 9009053696.0000 - accuracy: 0.0000e+00\n",
      "Epoch 15/20\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 9009018880.0000 - accuracy: 0.0000e+00\n",
      "Epoch 16/20\n",
      "50/50 [==============================] - 0s 979us/step - loss: 9008984064.0000 - accuracy: 0.0000e+00\n",
      "Epoch 17/20\n",
      "50/50 [==============================] - 0s 950us/step - loss: 9008945152.0000 - accuracy: 0.0000e+00\n",
      "Epoch 18/20\n",
      "50/50 [==============================] - 0s 859us/step - loss: 9008912384.0000 - accuracy: 0.0000e+00\n",
      "Epoch 19/20\n",
      "50/50 [==============================] - 0s 878us/step - loss: 9008877568.0000 - accuracy: 0.0000e+00\n",
      "Epoch 20/20\n",
      "50/50 [==============================] - 0s 957us/step - loss: 9008842752.0000 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0897544898>"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add( SimpleRNN(4 , input_shape=(1 , 1)) )\n",
    "\n",
    "model.add( Dense(1) )\n",
    "\n",
    "model.compile(loss=\"mse\" , optimizer=\"adam\" , metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(clipped_train[0] , clipped_test[0] , epochs=20 , batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
